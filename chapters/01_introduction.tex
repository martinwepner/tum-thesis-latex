\chapter{Introduction}\label{chapter:introduction}
The simulation of fluids like the behavior of water in a waterfall or smoke ascending from fire has become a key element of visual effects in the past years increasing the immersion of movies and computer games. Two main approaches have arisen to achieve realistic results: The Lagrangian method, which computes the dynamics of individual particles in respect to each other and the Eulerian method, that uses a fixed grid and computes how mass is moved within the grid. Existing solutions are often highly computational expensive leading to trade-offs between speed and accuracy, density or size and thus believability of the simulation.\par
Some recent approaches tackling this issue by leveraging the power of Deep Neural Networks and transforming it into a data-driven unsupervised learning problem. Until recently only Eulerian methods were fully differentiable making them suitable for deep-learning. Using simulation data such as velocity and density fields of each timestep, Deep Neural Networks can be trained to mimic the concepts of the Navier-Stokes equation [Paper of Jonathan Tompson, Ken Perlin].\par
For larger simulations, the amount of physical memory needed to store the training data gets huge quickly. To avoid this, running data generation and the training process itself can be done in parallel: After every time-step is calculated the network's weights can be adjusted.
\section{$\Phi_\textit{Flow}$ }
$\Phi_\textit{Flow}$ is a toolkit written in Python and currently under development at the TUM Chair of Computer Graphics and Visualization for solving and visualizing n-dimensional fluid simulations. It focuses on keep every simulation step differentiable which makes it suitable for backpropagation. Backpropagation on the other hand is required to calculate the gradient of the loss function of Neural Networks to adjust the weight of the network. Being developed on top of TensorFlow it is not only capable to run on CPU, but also completely on GPU. TensorFlow simplifies the implementation of Machine Learning algorithms including the use of Neural Networks for predictions and classification. This makes $\Phi_\textit{Flow}$ a powerful tool to train Neural Networks on fluid simulations and also visualize them by its interactive GUI running in the browser. 
\section{Using TensorFlow Custom Operations for efficient computation}
This work proposes an implementation to solve the pressure equation - the most computational intensive part of Eulerian fluid simulations. Being as efficient as possible is crucial for fast simulations and thus most importantly for the training of neural networks. $\Phi_\textit{Flow}$ already comes with its own pressure solver running directly in TensorFlow. However, it is unnecessarily complex how TensorFlow builds the dataflow graph exactly which brings overhead to the computation.\par
TensorFlow offers the possibility to write custom operations ("Custom ops") in C++ to tackle this issue: Writing efficient code to solve a specific problem within the dataflow graph. Furthermore, the CUDA API by Nvidia makes it possible to write low-level CUDA code that runs natively on GPUs. Combining Custom Ops and CUDA kernels leads to a solution that is both efficient and transparent and also compliant to TensorFlow's Tensors which is required to be used within $\Phi_\textit{Flow}$. It can then be compared to $\Phi_\textit{Flows}$ built-in solution.
\section{Pressure Solve and Conjugate Gradient}
The Pressure Solve Problem can be expressed as a system of linear equations whose solution is the exact pressure value of each cell in order to be compliant with the incompressibility constraint of fluids. The matrix of the linear system is symmetric and positive-definite which makes it suitable to be solved by the Conjugate Gradient method. GPUs are well suited for parallel computations, especially linear algebra. The cg-method consists of only some vector operations and matrix-vector multiplications and can therefore be run on GPUs. Being a sparse matrix gives additional room for improvement, by not only keeping the required memory low but also decreasing the amount of data, that needs to be sent between the GPU's global memory to the on-chip local memory and vice versa. Compared to computation, memory operations are very costly. By not changing the boundary conditions of the simulation the matrix stays constant. Very expensive transfers between CPU and GPU can therefore kept low.