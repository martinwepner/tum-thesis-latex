\chapter{CUDA Pressure Solver}\label{chapter:cudapressuresolver}
In this chapter, we find an efficient numerical approach to solve the system of linear equations $\mathbf{A}\vec{p} = \frac{\rho \Delta x^2}{\Delta t}\nabla \cdot \vec{u}^n$ (\ref{pressure-equation-shortened}) using TensorFlows Custom Ops and Nvidia CUDA Kernels. First, we discuss how \textbf{A} is built and stored in memory and then how the Conjugate Gradient method can be used to solve it. 
\par We will use some recurring variables which I clarify here:
\begin{itemize}
	\setlength\itemsep{-0.3em}
	\item \textit{dimensions} is an array which holds the size of each dimension of the grid.
	\item \textit{dimSize} holds the number of dimensions.
	\item \textit{dimProduct} is the product $\prod_{i=0}^{dimSize} \textit{dimensions}[i]$ of all dimensions. 
	\item \textit{mask} holds the information of fluid, air and solid cells including the boundary cells.
	\item \textit{maskDimensions} is an array that holds the dimensions of the mask.
\end{itemize}

\section{GPU Laplace Matrix generation}
The goal of this section is to find a parallelizable algorithm that creates the Laplace Matrix A, so that a GPU can compute exactly one cell $(i,j)$ or one row of \textbf{A} per thread.
\par GPU memory is very limited. For a simulation of the size 64 x 64 x 64 the dimension of the Laplace Matrix is 262.144 x 262.144. This would require 256 Gigabyte of GPU-memory if we tried to cache the matrix with 32 bit floats.  
\par Take a look back on the example Laplace Matrix in Fig. \ref{fig:laplace-matrix}. Notice, that most of it is zero. In fact, there are only at most as many non-zero entries as the number of neighbors of a cell plus one for the cell itself:
\begin{equation}
	maxNonZerosPerRow = dimSize * 2 + 1
\end{equation}
For 2D this means at most five and for 3D at most seven entries per row.
\newpage
\begin{figure*}
	\centering
	\begin{tabular}{lll}
	\multirow{3}{*}{$\left( {\begin{array}{cccccc}
   4 & 0 & 0 & 0 & 0 & 0  \\
   1 & 4 & 0 & 0 & 0 & 0  \\
   0 & 0 & 4 & 0 & 0 & 0  \\
   3 & 0 & 0 & 4 & 0 & 0  \\
  \end{array} } \right)$}
	& \hyperref[csr-data]{\textit{data}} & $=[4,1,4,4,3,4]$  \\
	& \hyperref[csr-columnptr]{\textit{columnptr}} & $=[0,0,1,2,0,3]$ \\
	& \hyperref[csr-rowcnt]{\textit{rowcnt}} & $=[0,1,3,4,6]$ \\
	& & 
	\end{tabular}
\caption{Example Matrix in CSR format}\label{fig:csr-matrix}
\end{figure*}
\par The \textit{compressed sparse row} (CSR) \parencite{tinney1967direct} format tackles this memory problem by only storing information of non-zero entries of a sparse matrix. It consists of three arrays:\\\\
\begin{tabular}{ll}
	\textit{data\label{csr-data}}: & values of non-zero entries in row-major order.\\
	\textit{columnptr\label{csr-columnptr}}: & the column indices of all non-zero data entries in row-major order.\\
	\textit{rowcnt\label{csr-rowcnt}}: & The $i$th element points to the index in \hyperref[csr-data]{\textit{data}} of the first non-zero entry of the $i$th row.
\end{tabular}\\\\
Because \textbf{A} is sparse to a vast extent, the CSR format reduces memory consumption dramatically. However, it requires three memory accesses to get all data of one entry. Compared to pure computation, memory accesses are very costly on GPUs and should be kept as low as possible \parencite{fang2018benchmarking}\parencite{fujii2013data}. To improve speed, reducing those memory accesses is crucial.

[TODO: Insert naive algorithm here]
\newpage


\subsection{Optimization \rom{1}: Abandonment of CSR \hyperref[csr-rowcnt]{\textit{rowcnt}} array}\label{optimization1}
For every row $i$ we know that all non-zero entries of this row lay in the range $\big[\text{\hyperref[csr-rowcnt]{\textit{rowcnt}}[i] ; \hyperref[csr-rowcnt]{\textit{rowcnt}}[i+1]}\big[$ in the \hyperref[csr-data]{\textit{data}} array. 
\par The maximum number of non-zero entries per row in the Laplace Matrix is $maxNonZerosPerRow$ as defined above. It is only the maximum possible number of non-zeros because neighbor cells of $(i,j)$ might be open or solid cells, thus zero in the $(i,j)$th row of the Laplace Matrix. For cells at the edge of the grid, we set the corresponding value for neighbor cells that lay outside the grid to zero in the \hyperref[csr-data]{\textit{data}} array as well. The CSR format will remove those zero entries but if we keep them, we have a regular arrangement of \hyperref[csr-data]{\textit{data}} entries per row. As a trade-of this will require slightly more memory. However now we are able calculate the index of the row $i_{row}$ of any \hyperref[csr-data]{\textit{data}} entry by its index $i_{data}$:
\begin{equation} \label{row-index-by-data-index}
	i_{row} = \floor*{\frac{i_{data}}{maxNonZerosPerRow}}
\end{equation}
\par By getting rid of the \hyperref[csr-rowcnt]{\textit{rowcnt}} array we not only reduce memory accesses, it was also required to parallelize the Laplace Matrix generation algorithm. When a row in \textbf{A} for one cell $(i,j)$ is built we need to make sure, that the order of the data is inserted in row major order. If the GPU executes two cells for example $(i+1,j)$ and $(i,j)$ at the same time, the \hyperref[csr-data]{\textit{data}} array would violate the insertion order rule. With the regular arrangement every thread or cell has a pre-defined range in which it will insert the data.
\newpage

\begin{figure*}
\[
  merged = \underbrace{\underbrace{\;0\,0\,0\,0\:\:1\,1\,1\,1\;\;0\,0\,0\,0\:\:1\,1\,1\,1\;\;0\,0\,0\,0\:\:1\,1\,1\,1\;}_\text{\hyperref[csr-columnptr]{\textit{columnptr}}, 24 bit}\;\;\underbrace{\;0\,0\,0\,0\:\:1\,1\,1\,1\;}_\text{\hyperref[csr-data]{\textit{data}}, 8 bit}}_{INT32}
\]
\caption{\hyperref[csr-data]{\textit{data}} and \hyperref[csr-columnptr]{\textit{columnptr}} entry merged into one $INT32$}
\label{fig:data-columnptr-merge}
\end{figure*} 

\subsection{Optimization \rom{2}: Merge of CSR \hyperref[csr-data]{\textit{data}} and \hyperref[csr-columnptr]{\textit{columnptr}} arrays}
Another observation we can make from equation \ref{pressure-equation} and Fig. \ref{fig:laplace-matrix} is that the values of the \hyperref[csr-data]{\textit{data}} array are integers in the range [$-2 \cdot dimSize$ ; 1]. Using an $INT32$ is a waste of memory. Instead an $INT8$ is more than enough (except in the unlikely case that you want to simulate more than 127 dimensions). To reduce memory accesses we can merge pairs of \hyperref[csr-data]{\textit{data}} and \hyperref[csr-columnptr]{\textit{columnptr}} into one single $INT32$ as stated in Fig \ref{fig:data-columnptr-merge}. 
\par Now writing a pair of \hyperref[csr-data]{\textit{data}} and \hyperref[csr-columnptr]{\textit{columnptr}} requires to shift \hyperref[csr-columnptr]{\textit{columnptr}} 8 bit to the left and add  \hyperref[csr-data]{\textit{data}} using bitwise OR masking.
\par Reading is similar: For \hyperref[csr-columnptr]{\textit{columnptr}} first remove \hyperref[csr-data]{\textit{data}} from $merged$ using bitwise AND with 0xFFFFFF00 then shift the result 8 bit to the right. To read \hyperref[csr-data]{\textit{data}}, mask off \hyperref[csr-columnptr]{\textit{columnptr}} from $merged$ with the 0x000000FF.
\par Now we reduced the amount of memory reads for one matrix entry from three to one. However, this optimization comes with a major drawback: \hyperref[csr-columnptr]{\textit{columnptr}} only has 24 bit left to address the column indices of the Laplace Matrix. Remember, the Laplace matrix correlates all cells, meaning we need $dimProduct$ columns and rows. So this optimization is only compliant for simulations with $dimProduct \leq 2^{24}$ or for example a grid of the size 256 x 256 x 256. In the next section we learn that we can even get rid of \hyperref[csr-columnptr]{\textit{columnptr}} at all.
\newpage
\begin{figure*}
\centering
\[
	\hyperref[csr-data]{\textit{data}} = \left[ 
	\underbrace{\text{ \textcolor{brown}{0}, \textcolor{brown}{0}, \textcolor{red}{-2}, \textcolor{blue}{1}, \textcolor{blue}{1}}}_{i_{row} = 0} \text{ , }
	\underbrace{\text{ \textcolor{brown}{0}, \textcolor{blue}{0}, \textcolor{red}{-3}, \textcolor{blue}{0}, \textcolor{blue}{1}}}_{i_{row} = 1} \text{ , }
	\underbrace{\text{ \textcolor{brown}{0}, \textcolor{blue}{0}, \textcolor{red}{-3}, \textcolor{blue}{0}, \textcolor{blue}{1}}}_{i_{row} = 2} \text{ , }
	\underbrace{\text{ \textcolor{brown}{0}, \textcolor{blue}{0}, \textcolor{red}{-2}, \textcolor{brown}{0}, \textcolor{blue}{0}}}_{i_{row} = 3} \text{ , }
	\underbrace{\text{ \textcolor{blue}{0}, \textcolor{brown}{0}, \textcolor{red}{-3}, \textcolor{blue}{1}, \textcolor{blue}{0}}}_{i_{row} = 4} \text{ , }
	\dots
	\right]
\]
\caption{Vizualization of \hyperref[csr-data]{\textit{data}} array from Fig. \ref{fig:laplace-matrix}. Diagonal entries are printed in red. Neighbor cells are printed in blue if they are within the grid and brown otherwise.
}
\label{fig:data-columnptr-merge}
\end{figure*} 

\subsection{Optimization \rom{3}: Abandonment of CSR \hyperref[csr-columnptr]{\textit{columnptr}}}
More in-depth inspection of Fig. \ref{fig:laplace-matrix} reveals that non-zeros always appear by the same distances to the diagonal of each row. Of course, this is due to the fact that the respective neighbors on the linearized grid lie at the same distance from each other for every cell accordingly. Because of Optimization \rom{1} (\ref{optimization1}) and because we insert the matrix entries in row-major format, this regularity is also applicable for the \hyperref[csr-data]{\textit{data}} array. With this knowledge we can extract a method to infer from each \hyperref[csr-data]{\textit{data}} entry the respective column index $i_{column}$ via the data index $i_{data}$. 
\par 



\par [\dots]
\par => Offset array zum Diagonal berechnen z.b. (-64, -1, 0, 1, 64) und damit dann $i_{column}$ ableiten

\newpage

\section{GPU Pressure Solve}
In the previous section we found an algorithm that creates and stores the Laplace Matrix efficiently on a GPUs. In this section I present how the system of linear equations $\mathbf{A}\vec{p} = \frac{\rho \Delta x^2}{\Delta t}\nabla \cdot \vec{u}^n$ (\ref{pressure-equation-shortened}) can be solved on GPUs using the Conjugate Gradient method. First, we simplify equation \ref{pressure-equation-shortened}
\begin{equation}
	\mathbf{A}\vec{p} = \vec{d} 
\end{equation}
Now solving this system of linear equations with the cg-method works as follows

\begin{algorithm}
\caption{Pressure Solve with cg-method}\label{pressure-solve-cg}
\begin{algorithmic}[1]
\Function {Solve Pressure}{\textbf{A}, $\vec{d}$, $maxIterations$, $accuracy$ }
	\State $\vec{m} \gets \vec{d}$ \Comment{Init helper variables}
	\State $\vec{r} \gets \vec{d}$
	\State $\vec{p} \gets 0$ \Comment{TODO initial guess?}
	\For{$k \gets 1$ to $maxIterations$}
		\State $\vec{z} \gets \mathbf{A} \vec{m}$ \label{calcZ} \Comment{calcZ}
		\State $alpha \gets \frac{\vec{m} \bullet \vec{r}}{\vec{m} \bullet \vec{z}}$
		\State $\vec{p} \gets \vec{p} + alpha \cdot \vec{m}$
		\State $\vec{r} \gets \vec{r} - alpha \cdot \vec{z}$
		\If {$ max_i(|\vec{r}_i|) < accuracy $} \label{checkResiduum} \Comment{Check residuum} 
			\State \Return $\vec{p}$
		\EndIf
		\State $beta \gets - \frac{\vec{r} \bullet \vec{z}}{\vec{m} \bullet \vec{z}}$
		\State $\vec{m} \gets \vec{r} + beta \cdot \vec{m}$
	\EndFor

\State \Return $\vec{p}$
\EndFunction
\end{algorithmic}
\end{algorithm}
All mathematical operations but operation \ref{calcZ} and \ref{checkResiduum} can be executed by the highly optimized cuBLAS library which is part of CUDAs standard libraries. \\
Obviously operation \ref{calcZ} is the most demanding because it involves a matrix-vector multiplication. Naively we would have to sum $dimProduct$ multiplications per entry of $\vec{z}$.

\newpage
\begin{algorithm}
\caption{My algorithm}\label{euclid}
\begin{algorithmic}[1]
\Procedure{MyProcedure}{1,2,3}
\State $\textit{stringlen} \gets \text{length of }\textit{string}$
\State $i \gets \textit{patlen}$
\BState \emph{top}:
\If {$i > \textit{stringlen}$} \Return false
\EndIf
\State $j \gets \textit{patlen}$
\BState \emph{loop}:
\If {$\textit{string}(i) = \textit{path}(j)$}
\State $j \gets j-1$.
\State $i \gets i-1$.
\State \textbf{goto} \emph{loop}.
\State \textbf{close};
\EndIf
\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
\State \textbf{goto} \emph{top}.
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithmic}
\If {$i\geq maxval$}
    \State $i\gets 0$
\Else
    \If {$i+k\leq maxval$}
        \State $i\gets i+k$
    \EndIf
\EndIf
\end{algorithmic}