\chapter{CUDA Pressure Solver}\label{chapter:cudapressuresolver}
In this chapter, we find an efficient numerical approach to solve the system of linear equations $\mathbf{A}\vec{p} = \frac{\rho \Delta x^2}{\Delta t}\nabla \cdot \vec{u}^n$ (\ref{pressure-equation-shortened}) using TensorFlows Custom Ops and Nvidia CUDA Kernels. First, we discuss how \textbf{A} is built and stored in memory and then how the Conjugate Gradient method can be used to solve it. 
\par We will use some recurring variables which I clarify here:
\begin{itemize}
	\setlength\itemsep{-0.3em}
	\item \textit{dimensions} is an array which holds the size of each dimension of the grid.
	\item \textit{dimSize} holds the number of dimensions.
	\item \textit{dimProduct} is the product $\prod_{i=0}^{dimSize} \textit{dimensions}[i]$ of all dimensions. 
	\item \textit{mask} holds the information of fluid, open and solid cells including the boundary cells.
	\item \textit{maskDimensions} is an array that holds the dimensions of the mask.
\end{itemize}

\section{GPU Laplace Matrix generation}
The goal of this section is to find a parallelizable algorithm that creates the Laplace Matrix A, so that a GPU can compute exactly one cell $(i,j)$ or one row of \textbf{A} per thread.
\par GPU memory is very limited. For a simulation of the size 64 x 64 x 64 the dimension of the Laplace Matrix is 262.144 x 262.144. This would require 256 GB of GPU-memory if we tried to cache the matrix with 32 bit numbers. At the time of this writing this is only possible with special GPUs that have additional GPU-SSD memory.
\par Take a look back on the example Laplace Matrix in Fig. \ref{fig:laplace-matrix}. Notice, that most of it is zero. In fact, there are only at most as many non-zero entries as the number of neighbors of a cell plus one for the cell itself:
\begin{equation}
	maxNonZerosPerRow = dimSize * 2 + 1
\end{equation}
For 2D this means at most five and for 3D at most seven entries per row.
\newpage
\begin{figure*}
	\centering
	\begin{tabular}{lll}
	\multirow{3}{*}{$\left( {\begin{array}{cccccc}
   4 & 0 & 0 & 0 & 0 & 0  \\
   1 & 4 & 0 & 0 & 0 & 0  \\
   0 & 0 & 4 & 0 & 0 & 0  \\
   3 & 0 & 0 & 4 & 0 & 0  \\
  \end{array} } \right)$}
	& \hyperref[csr-data]{\textit{data}} & $=[4,1,4,4,3,4]$  \\
	& \hyperref[csr-columnptr]{\textit{columnptr}} & $=[0,0,1,2,0,3]$ \\
	& \hyperref[csr-rowcnt]{\textit{rowcnt}} & $=[0,1,3,4,6]$ \\
	& & 
	\end{tabular}
\caption{Example Matrix in CSR format}\label{fig:csr-matrix}
\end{figure*}
\par The \textit{compressed sparse row} (CSR) format tackles this memory problem by only storing information of non-zero entries of a sparse matrix \parencite{tinney1967direct}. It consists of three arrays:\\\\
\begin{tabular}{ll}
	\textit{data\label{csr-data}}: & values of non-zero entries in row-major order.\\
	\textit{columnptr\label{csr-columnptr}}: & the column indices of all non-zero data entries in row-major order.\\
	\textit{rowcnt\label{csr-rowcnt}}: & The $i$th element points to the index in \hyperref[csr-data]{\textit{data}} of the first non-zero entry of the $i$th row.
\end{tabular}\\\\
Because \textbf{A} is sparse to a vast extent, the CSR format reduces memory consumption dramatically. However, it requires three memory accesses to get all data of one entry. Compared to pure computation, memory accesses are very costly on GPUs and should be kept as low as possible \parencite{fang2018benchmarking}\parencite{fujii2013data}. To improve speed, reducing those memory accesses is crucial.

\subsection{Tailoring the CSV format for the GPU pressure solve}\label{laplace-optimization}
For every row $i$ we know that all non-zero entries of this row lay in the range $\big[\text{\hyperref[csr-rowcnt]{\textit{rowcnt}}[i] ; \hyperref[csr-rowcnt]{\textit{rowcnt}}[i+1]}\big[$ in the \hyperref[csr-data]{\textit{data}} array. 
\par The maximum number of non-zero entries per row in the Laplace Matrix is $maxNonZerosPerRow$ as defined above. It is only the maximum possible number of non-zeros because neighbor cells of $(i,j)$ might be open or solid cells, thus zero in the $(i,j)$th row of the Laplace Matrix. For cells at the edge of the grid, we set the corresponding value for neighbor cells that lay outside the grid to zero in the \hyperref[csr-data]{\textit{data}} array as well. The CSR format would have removed those zero entries but if we keep them, we have a regular arrangement of \hyperref[csr-data]{\textit{data}} ($data_{regular}$)-entries per row. As a trade-of this will require slightly more memory. However we are now able to calculate the index of the row $i_{row}$ of any \hyperref[csr-data]{$data_{regular}$} entry by its index $i_{data}$:
\begin{equation} \label{row-index-by-data-index}
	i_{row} = \floor*{\frac{i_{data}}{maxNonZerosPerRow}}
\end{equation}
\par For efficient parallelizing, this step is indispensable. If the GPU executes two cells for example $(i+1,j)$ and $(i,j)$ in two threads at the same time and every \hyperref[csr-data]{\textit{data}} entry does not have a predefined place in the array, the threads would mix up their insertions and thus the row-major structure. Since $data_{regular}$ is now regular, each thread knows the exact position in $data_{regular}$ for every entry to comply the row-major order.
\newpage

\begin{figure*}
\centering
\[
	\hyperref[csr-data]{\textit{data}} = \left[ 
	\underbrace{\text{ \textcolor{brown}{0}, \textcolor{brown}{0}, \textcolor{red}{-2}, \textcolor{blue}{1}, \textcolor{blue}{1}}}_{i_{row} = 0} \text{ , }
	\underbrace{\text{ \textcolor{brown}{0}, \textcolor{blue}{0}, \textcolor{red}{-3}, \textcolor{blue}{0}, \textcolor{blue}{1}}}_{i_{row} = 1} \text{ , }
	\underbrace{\text{ \textcolor{brown}{0}, \textcolor{blue}{0}, \textcolor{red}{-3}, \textcolor{blue}{0}, \textcolor{blue}{1}}}_{i_{row} = 2} \text{ , }
	\underbrace{\text{ \textcolor{brown}{0}, \textcolor{blue}{0}, \textcolor{red}{-2}, \textcolor{brown}{0}, \textcolor{blue}{0}}}_{i_{row} = 3} \text{ , }
	\underbrace{\text{ \textcolor{blue}{0}, \textcolor{brown}{0}, \textcolor{red}{-3}, \textcolor{blue}{1}, \textcolor{blue}{0}}}_{i_{row} = 4} \text{ , }
	\dots
	\right]
\]
\caption{Vizualization of $data_{regular}$ array for Fig. \ref{fig:laplace-matrix}. Diagonal entries are printed in red. Neighbor cells are printed in blue if they are within the grid and brown otherwise.
}
\label{fig:data-columnptr-merge}
\end{figure*} 
\begin{figure*}
\centering
\[
	diagonalOffset = \left[ \text{ -4, -1, \textcolor{red}{0}, 1, 4 } \right]
\]
\caption{$diagonalOffset$ array that corresponds to Fig. \ref{fig:laplace-matrix}.}
\label{fig:data-columnptr-merge}
\end{figure*} 
More in-depth inspection of Fig. \ref{fig:laplace-matrix} reveals that neighbor-cell entries always appear by the same distances to the diagonal of each row. Of course, this is due to the fact that the respective neighbors on the linearized grid lie at the same distance from each other for every cell accordingly. This is also applicable to the $data_{regular}$ array: in respect to the diagonal (red) entries the neighbors always appear in the same order (Fig. \ref{fig:data-columnptr-merge}). With this knowledge we can extract a method to infer from each $data_{regular}$ entry the respective column index $i_{column}$ via the data index $i_{data}$. 
\par To do this, we define a new helper array $diagonalOffsets$ which stores the column-index distance to the diagonal of every entry of a row.
Here is an algorithm to create the $diagonalOffsets$ array:

\begin{algorithm}
\caption{Creates an array containing the column offset for every neighbor of a row in \textbf{A} in respect to the diagonal}\label{diagonalOffsets}
\begin{algorithmic}[1]
\Function {Get diagonalOffsets}{$maxDataPerRow$, $dimensions$ }
	\State $offsets = \left[ i_0, i_1, \cdots, i_{maxDataPerRow} \right]$  \Comment{Declare array of length $maxDataPerRow$}
	\State $factor = 1$
	\State $\mathbf{const}$ $diagonal = \floor*{\frac{maxNonZerosPerRow}{2}}$
	\State 
	\For{$i = 0,$ $offset = 1;$ $i < diagonal;$ $i$++$,$ $offset$++}
    	\State $offsets[diagonal - offset] = -factor$
        \State $offsets[diagonal + offset] = factor$
        \State $factor = factor * dimensions[i]$
	\EndFor
	\State $offsets[diagonal] = 0$
	\State \Return $offsets$
	
\EndFunction
\end{algorithmic}
\end{algorithm}
Now getting the column index $i_{column}$ by an arbitrary $data_{regular}$ index $i_{data}$ is trivial:
\begin{equation}
\begin{aligned}
	i_{column} & = \floor*{\frac{i_{data}}{maxNonZerosPerRow}} & + columnOffsets\left[ i_{data} \mod maxNonZerosPerRow \right]\\
			   & = i_{row} & + columnOffsets\left[ i_{data} \mod maxNonZerosPerRow \right]\\
\end{aligned}
\end{equation}


\subsection{Create the Laplace Matrix in the Tailored CSV format on the GPU}\label{laplace-creation}
We now derive an algorithm, that creates the Laplace Matrix in parallel on the GPU. We apply the Tailored CSV format that was introduced in the previous section. 
\par The goal is, that every GPU-thread will compute one cell or one row in \textbf{A}. To do that, we start  $dimProduct$ threads each providing the $rowID$ by its thread id. Using the $rowID$ we have to check the state in $mask$ of all neighbors and insert the data accordingly in row-major format. To do that, we first review the neighbors that appear before the current cell in the linearized grid and push their values, add the diagonal entry of the current cell itself and then check the remaining neighbors and add their data:

\begin{algorithm}
\caption{Create Laplace Matrix }\label{laplace-matrix-generation}
\begin{algorithmic}[1]
\Procedure {Get Laplace Matrix}{$rowID$, $mask$, $data_{regular}$ }
	\State $i_{data} = rowID \cdot (dimSize \cdot 2 + 1)$
	\State $diagonalValue = - dimSize * 2$
	
	\State
	\State \Comment{Insert data of neighbors that appear before the current cell in linearized grid}
	\For{$neighbor$ of $beforeNeighbors$ in row-major}
		\If {$neighbor$ is Fluid Cell in $mask$ } 
			\State $data_{regular}[i_{data}] = 1$
		\ElsIf {$neighbor$ is Solid Cell in $mask$}
			\State $data_{regular}[i_{data}] = 0$
			\State $diagonalValue$++
		\ElsIf {$neighbor$ is Open Cell in $mask$}
			\State $data_{regular}[i_{data}] = 0$
		\EndIf
		\State $i_{data}$++
	\EndFor
	
	\State
	\State \Comment{Modify $diagonalValue$ according to the $nextNeighbors$}
	\For{$neighbor$ of $afterNeighbors$ in row-major}
		\If {$neighbor$ is Solid Cell in $mask$}
			\State $diagonalValue$++
		\EndIf
	\EndFor
	
	\State
	\State \Comment{Insert diagonal entry of the current cell}
	\State $data_{regular}[i_{data}] = diagonalValue$
	\State $i_{data}$++
	
	\State
	\State \Comment{Insert data of neighbors that appear after the current cell in linearized grid}
	\For{$neighbor$ of $afterNeighbors$ in row-major}
		\If {$neighbor$ is Fluid Cell in $mask$ } 
			\State $data_{regular}[i_{data}] = 1$
		\ElsIf {$neighbor$ is Solid Cell in $mask$}
			\State $data_{regular}[i_{data}] = 0$
		\ElsIf {$neighbor$ is Open Cell in $mask$}
			\State $data_{regular}[i_{data}] = 0$
		\EndIf
		\State $i_{data}$++
	\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}



\newpage
\section{GPU Pressure Solve}
In the previous section we found an algorithm that creates and stores the Laplace Matrix \textbf{A} efficiently on GPUs. In this section I present how the system of linear equations $\mathbf{A}\vec{p} = \frac{\rho \Delta x^2}{\Delta t}\nabla \cdot \vec{u}^n$ (\ref{pressure-equation-shortened}) can be solved on GPUs using the Conjugate Gradient method. First, we simplify equation \ref{pressure-equation-shortened}:
\begin{equation}
	\mathbf{A}\vec{p} = \vec{d} 
\end{equation}
Now solving this system of linear equations with the cg-method works as follows \parencite{hestenes1952methods}:

\begin{algorithm}
\caption{Pressure Solve with cg-method}\label{pressure-solve-cg}
\begin{algorithmic}[1]
\Function {Solve Pressure}{\textbf{A}, $\vec{d}$, $maxIterations$, $accuracy$, $\vec{p}_{0}$ }
	\State $\vec{m} = \vec{r} = \vec{d} - \mathbf{A} \vec{p}_{0}$ \Comment{Init helper variables}
	\State $\vec{p} = \vec{p}_{0}$
	\For{$k = 1$ to $maxIterations$}
		\State $\vec{z} = \mathbf{A} \vec{m}$ \label{calcZ} \Comment{calcZ}
		\State $alpha = \frac{\vec{m} \bullet \vec{r}}{\vec{m} \bullet \vec{z}}$
		\State $\vec{p} = \vec{p} + alpha \cdot \vec{m}$
		\State $\vec{r} = \vec{r} - alpha \cdot \vec{z}$
		\If {$ max_i(|\vec{r}_i|) < accuracy $} \label{checkResiduum} \Comment{checkResiduum} 
			\State \Return $\vec{p}$
		\EndIf
		\State $beta = - \frac{\vec{r} \bullet \vec{z}}{\vec{m} \bullet \vec{z}}$
		\State $\vec{m} = \vec{r} + beta \cdot \vec{m}$
	\EndFor

\State \Return $\vec{p}$
\EndFunction
\end{algorithmic}
\end{algorithm}
All mathematical operations except for operation \ref{calcZ} and \ref{checkResiduum} can be executed by the highly optimized cuBLAS library which is part of CUDAs standard libraries. \\
Obviously operation \ref{calcZ} is the most demanding because it involves a matrix-vector multiplication. Now we can take advantage of the findings and optimization steps in the previous section. 


\newpage
\subsubsection{\hyperref[calcZ]{calcZ}}
Without further ado, we can derive an algorithm to calculate $\vec{z}$ using \textbf{A} in the Tailored CSR format. Every thread computes one entry of the resulting vector $\vec{z}$. The row $rowID$ of this vector is given by the CUDA thread and block id:
\begin{algorithm}
\caption{Multiplies \textbf{A} with a vector}\label{pressure-solve-calc-z}
\begin{algorithmic}[1]
\Function {calcZ}{$rowID$, \textbf{A}, $\vec{m}$, $\vec{z}$, $columnOffsets$ }
	\If {$rowID < dimProduct$} 
		\State $rowStartIndex = rowID * maxNonZerosPerRow$ 
		\State $tmp = 0$ 
		\For{$i = rowStartIndex;$ $i < rowStartIndex + maxNonZerosPerRow;$ $i$++}
			\State $tmp = tmp + \mathbf{A}[i] \cdot \vec{m}[rowID + columnOffsets[i - rowStartIndex]]$
		\EndFor
		\State $\vec{z}[rowID] = tmp$
	\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{\hyperref[checkResiduum]{checkResiduum}}
The cg-method approximates the solution of a system of linear equations iteratively. It should terminate if all entries of the residuum vector $\vec{r}$ is smaller than the given $accuracy$. We introduce a new variable $finished$ and set it to true at the beginning of every cg-iteration. The \hyperref[checkResiduum]{checkResiduum} method searches for an entry in $\vec{r}$ to be greater than $accuracy$ in parallel. Every match sets $finished$ to false. When all checks have been done we can break the cg-iteration if $finished$ is still true.



























