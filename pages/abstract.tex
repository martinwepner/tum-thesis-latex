\chapter{\abstractname}
\small
The numerical approximation of Navier-Stokes equations has been a topic of research for a long time. Solving the pressure equation to obtain incompressibility is computationally expensive. Newer approaches optimize this step by training neural networks based on fluid simulations. $\Phi_{Flow}$ is a toolkit that is based on the Machine Learning Platform TensorFlow and can simulate fluids fully differentiable. It uses the calculated data during the simulation to adjust the weights of NNs. Its pressure solver is implemented in TensorFlow and thus capable of running on CPUs and GPUs. However, TensorFlow's Dataflow Graph is unnecessarily complex, which slows down the simulation and thus the training of neural networks. 
\par This thesis addresses this performance problem by presenting a TensorFlow Custom Op that solves the pressure efficiently on the GPU. The tailored CSR format was presented, which efficiently stores the Laplace matrix of the pressure to minimize memory access. 
\par My solution outperforms $\Phi_{Flow}$s native TensorFlow implementation by a factor of up to two orders of magnitudes compared to the CPU version and is up to 70 times faster than the GPU version on my test environment.\\\\\\\\
Das numerische Approximieren der Navier-Stokes Gleichungen ist schon lange ein Thema der Forschung. Das Lösen der Druck Gleichung um Inkompressibilität zu erhalten ist sehr rechenintensiv. Neuere Ansätze optimieren diesen Schritt, indem sie Neuronale Netze auf Basis von Fluid-Simulationen trainieren. $\Phi_{Flow}$ ist ein Toolkit, dass auf der Machine Learning Platform TensorFlow aufbaut und Fluide voll differenzierbar simulieren kann. Es kann während der Simulation die berechneten Daten nutzen, um die Gewichte des NNs anzupassen. Der Druck Löser ist in TensorFlow implementiert und damit fähig, auf CPUs und GPUs zu laufen. TensorFlows Dataflow Graph ist aber unnötig komplex, was die Simulation verlangsamt und damit auch das Training der Neuronalen Netze. 
\par Diese Arbeit behandelt dieses Performance Problem, indem sie eine TensorFlow Custom Op präsentiert, die den Druck effizient auf der GPU löst. Hierzu wurde das Zugeschnittene CSR Format vorgestellt, das die Laplace Matrix des Drucks effizient speichert, um die Speicherzugriffe zu minimieren. 
\par Meine Lösung ist bis zu zwei Größenordnungen schneller als die native TensorFlow-Implementierung von $\Phi_{Flow}$ im Vergleich zur CPU-Version und bis zu 70 mal schneller als die GPU-Version in meiner Testumgebung.

\normalsize